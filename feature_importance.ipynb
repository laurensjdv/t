{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljdevries/.conda/envs/mridc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as func\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "from models.mlp import MLP\n",
    "from models.bin_mlp import binMLP\n",
    "# from dataloaders.batch_dataloader import FCMatrixDataset\n",
    "from dataloaders.dataloader import FCMatrixDataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "\n",
    "from utils import balanced_random_split_v2\n",
    "from copy import deepcopy\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GCN_train(model, loader, train_dataset, batch_size, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in loader:\n",
    "        data = data.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = func.cross_entropy(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "def MLP_train(model, loader, train_dataset, batch_size, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in loader:\n",
    "        inputs = data[0].to(DEVICE)\n",
    "        labels = data[1].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "       \n",
    "        loss = func.cross_entropy(output, labels)\n",
    "        loss.backward()\n",
    "        loss_all += batch_size * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "def MLP_test(model, loader, val_dataset, batch_size):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "    label = []\n",
    "    loss_all = 0\n",
    "    for data in loader:\n",
    "        inputs = data[0].to(DEVICE)\n",
    "        labels = data[1].to(DEVICE)\n",
    "\n",
    "        output = model(inputs)\n",
    "        loss = func.cross_entropy(output, labels)\n",
    "        loss_all += batch_size * loss.item()\n",
    "        # pred.append(func.softmax(output, dim=1).max(dim=1)[1])\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        pred.append(predicted)\n",
    "        label.append(labels)\n",
    "\n",
    "    y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "    y_true = torch.cat(label, dim=0).cpu().detach().numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(y_pred, y_true).ravel()\n",
    "    auc_score = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    epoch_acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "    return auc_score, epoch_acc, loss_all / len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: 001, Epoch: 025, Val Loss: 9.48188, Val BAC: 0.69231, Test BAC: 0.53046, TEST AUC: 0.53390\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 208\n",
    "dropout = 0.5140820967430961\n",
    "epochs = 25\n",
    "hidden_dim_ratio = 0.5\n",
    "layer_size = 256\n",
    "learning_rate = 0.0006834901970109743\n",
    "n_layers = 1\n",
    "weight_decay = 0.001\n",
    "input_features = 1485\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():  # GPU seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "ds = \"data/csv/severe_rds.csv\"\n",
    "data_dir = \"data/fetched/25751\"\n",
    "mrmr_features = None\n",
    "\n",
    "hidden_dims  = [int(layer_size * hidden_dim_ratio*(2**i)) for i in range(1, n_layers+1)]\n",
    "\n",
    "udi = data_dir.split(\"/\")[-1]\n",
    "data_dir = data_dir + \"/raw\"\n",
    "labels = np.genfromtxt(ds)\n",
    "labels = labels[1:, 1]\n",
    "dataset = FCMatrixDataset(ds, data_dir, udi, None, mrmr=mrmr_features)\n",
    "\n",
    "\n",
    "model = MLP(input_features, hidden_dims, 2, dropout).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=99)\n",
    "\n",
    "train_val, test = next(skf.split(labels, labels))\n",
    "\n",
    "\n",
    "train_val_dataset, test_dataset = Subset(dataset, train_val), Subset(dataset, test)\n",
    "train_val_labels = labels[train_val]\n",
    "train_val_index = np.arange(len(train_val_dataset))\n",
    "\n",
    "train, val, _, _ = train_test_split(train_val_index, train_val_labels, test_size=0.11, shuffle=True, stratify=train_val_labels)\n",
    "train_dataset, val_dataset = Subset(train_val_dataset, train), Subset(train_val_dataset, val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "min_v_loss = np.inf\n",
    "best_val_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    t_loss = MLP_train(model, train_loader, train_dataset, batch_size, optimizer)\n",
    "    _, val_acc, v_loss = MLP_test(model, val_loader, val_dataset, batch_size)\n",
    "\n",
    "    if min_v_loss > v_loss:\n",
    "    # if best_val_acc < val_acc:\n",
    "        min_v_loss = v_loss\n",
    "        best_val_acc = val_acc\n",
    "        best_model = deepcopy(model)\n",
    "test_auc, test_acc, _ = MLP_test(best_model, test_loader, test_dataset, batch_size)\n",
    "print('CV: {:03d}, Epoch: {:03d}, Val Loss: {:.5f}, Val BAC: {:.5f}, Test BAC: {:.5f}, TEST AUC: {:.5f}'.format(0 + 1, epoch + 1, min_v_loss, best_val_acc, test_auc,\n",
    "                                        test_acc))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1485, 236)\n",
      "(1485, 2)\n",
      "(1485, 2)\n",
      "[0.00983604 0.00961599]\n",
      "[1062  228]\n",
      "[[ 0.06873799  0.01801712]\n",
      " [-0.01835313  0.08292013]]\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import sklearn\n",
    "\n",
    "\n",
    "X = np.array([np.array(i[0]) for i in dataset])\n",
    "print(X.T.shape)\n",
    "X = torch.tensor(np.array([np.array(i[0]) for i in dataset])).to(DEVICE)\n",
    "# X = DataLoader(dataset, batch_size=48, shuffle=True)\n",
    "\n",
    "\n",
    "explainer = shap.DeepExplainer(best_model, X)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "print(shap_values[0].shape)\n",
    "print(shap_values[1].shape)\n",
    "\n",
    "# visualize the first prediction's explanation\n",
    "# shap.plots.waterfall(shap_values[0])\n",
    "# shap.summary_plot(shap_values, X)\n",
    "\n",
    "# shap.plots.bar(shap_values, max_display=10)\n",
    "\n",
    "score = np.mean(np.abs(shap_values[0]), axis=0)\n",
    "print(score)\n",
    "idx = np.argmax(shap_values[0], axis=0)\n",
    "print(idx)\n",
    "score2 = shap_values[0][idx]\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: 001, Epoch: 025, Val Loss: 8.33144, Val BAC: 0.61538, Test BAC: 0.83937, TEST AUC: 0.83898\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 208\n",
    "dropout = 0.5140820967430961\n",
    "epochs = 25\n",
    "hidden_dim_ratio = 0.5\n",
    "layer_size = 256\n",
    "learning_rate = 0.0006834901970109743\n",
    "n_layers = 1\n",
    "weight_decay = 0.001\n",
    "seed = 42\n",
    "\n",
    "input_features = 55\n",
    "mrmr_features = np.array([1140, 536, 223, 907, 1449, 499, 1293, 45, 135, 1440, 879, 1384, 1210, 1316, 122, 22, 492, 638, 765, 1027, 1464, 501, 1462, 395, 26, 1079, 70, 425, 1403, 1409, 1318, 886, 1459, 1448, 939, 1163, 547, 10, 413, 676, 131, 216, 942, 1136, 1386, 232, 1455, 1337, 814, 139, 392, 1376, 1382, 471, 656]\n",
    "        )\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():  # GPU seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "ds = \"data/csv/severe_rds.csv\"\n",
    "data_dir = \"data/fetched/25751\"\n",
    "\n",
    "hidden_dims  = [int(layer_size * hidden_dim_ratio*(2**i)) for i in range(1, n_layers+1)]\n",
    "\n",
    "udi = data_dir.split(\"/\")[-1]\n",
    "data_dir = data_dir + \"/raw\"\n",
    "labels = np.genfromtxt(ds)\n",
    "labels = labels[1:, 1]\n",
    "dataset = FCMatrixDataset(ds, data_dir, udi, mapping=None, mrmr=mrmr_features)\n",
    "\n",
    "\n",
    "model = MLP(input_features, hidden_dims, 2, dropout).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=99)\n",
    "\n",
    "train_val, test = next(skf.split(labels, labels))\n",
    "\n",
    "\n",
    "train_val_dataset, test_dataset = Subset(dataset, train_val), Subset(dataset, test)\n",
    "train_val_labels = labels[train_val]\n",
    "train_val_index = np.arange(len(train_val_dataset))\n",
    "\n",
    "train, val, _, _ = train_test_split(train_val_index, train_val_labels, test_size=0.11, shuffle=True, stratify=train_val_labels)\n",
    "train_dataset, val_dataset = Subset(train_val_dataset, train), Subset(train_val_dataset, val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "min_v_loss = np.inf\n",
    "best_val_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    t_loss = MLP_train(model, train_loader, train_dataset, batch_size, optimizer)\n",
    "    _, val_acc, v_loss = MLP_test(model, val_loader, val_dataset, batch_size)\n",
    "\n",
    "    if min_v_loss > v_loss:\n",
    "    # if best_val_acc < val_acc:\n",
    "        min_v_loss = v_loss\n",
    "        best_val_acc = val_acc\n",
    "        best_model = deepcopy(model)\n",
    "test_auc, test_acc, _ = MLP_test(best_model, test_loader, test_dataset, batch_size)\n",
    "print('CV: {:03d}, Epoch: {:03d}, Val Loss: {:.5f}, Val BAC: {:.5f}, Test BAC: {:.5f}, TEST AUC: {:.5f}'.format(0 + 1, epoch + 1, min_v_loss, best_val_acc, test_auc,\n",
    "                                        test_acc))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([236, 55])\n",
      "tensor([ 9.6208, -8.7138, -2.2577, -7.6152,  1.8930,  1.8710,  0.1609, -2.0138,\n",
      "        -0.1155, -2.0444,  3.1087, -1.8756,  0.9936,  1.7838, -2.7766,  2.9736,\n",
      "         2.6712, -7.5969,  0.1521, -2.2570, -1.7796,  1.7654,  0.9981,  3.2211,\n",
      "        -1.9279,  4.5055, 15.2677,  2.9986, -3.1585,  4.1001,  3.7219,  0.8900,\n",
      "        -3.2268, -3.5454,  7.5293,  4.6354,  6.9587, -2.0405, -7.3531, -0.7177,\n",
      "        -3.7782, -4.8360, -9.2442,  7.7194,  2.5564,  5.3103,  0.5323, -1.9091,\n",
      "         7.4302,  0.1335,  5.1687, -0.6371, -3.1070,  2.1771, -6.7912],\n",
      "       device='cuda:0')\n",
      "MLP(\n",
      "  (nn): ModuleList(\n",
      "    (0): Linear(in_features=55, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5140820967430961, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import sklearn\n",
    "\n",
    "\n",
    "X = torch.tensor(np.array([np.array(i[0]) for i in dataset])).to(DEVICE)\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "# X = DataLoader(dataset, batch_size=48, shuffle=True)\n",
    "\n",
    "\n",
    "explainer = shap.DeepExplainer(best_model, X)\n",
    "print(best_model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# get index of most important feature from shap_values\n",
    "score = np.mean(np.abs(shap_values[0]), axis=0)\n",
    "print(score)\n",
    "idx = np.argsort(shap_values[0], axis=0)[::-1][:5]\n",
    "idx = idx[:,0]\n",
    "score2 = shap_values[0][idx]\n",
    "print(score2)\n",
    "\n",
    "print(score2[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 25, 15, 10, 52])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mridc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
